{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be7001c8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f606001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab9e7542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", (tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a778c7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7c97879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0918e8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354084b6",
   "metadata": {},
   "source": [
    "## Load DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ef00f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>article</th>\n",
       "      <th>section_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b' we use the integral - field spectrograph sa...</td>\n",
       "      <td>b'according to the current galaxy formation pa...</td>\n",
       "      <td>b'introduction\\nsummary and conclusion\\nacknow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b\" we report on an effort to study the connect...</td>\n",
       "      <td>b\"the interaction between high power plasma je...</td>\n",
       "      <td>b'introduction\\ncomputational methods\\nmodel d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b' we consider the scenario of a magnetic fiel...</td>\n",
       "      <td>b'the interstellar medium ( ism ) exhibits str...</td>\n",
       "      <td>b'introduction\\nfront structure\\nphase transit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b' we present a model - independent analysis o...</td>\n",
       "      <td>b'accreting stellar - mass black holes in bina...</td>\n",
       "      <td>b'introduction\\nobservations and timing analys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b' we evaluate the feasibility of the implemen...</td>\n",
       "      <td>b'one of the most prominent applications in qu...</td>\n",
       "      <td>b'introduction\\nquantum repeater protocol opti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>b' clump clusters and chain galaxies in the hu...</td>\n",
       "      <td>b\"galaxies observed with the advanced camera f...</td>\n",
       "      <td>b'introduction\\ndata on udf bulges and clumps\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>b' recent advances in early detection and deta...</td>\n",
       "      <td>b\"within the fireball model for gamma - ray bu...</td>\n",
       "      <td>b'introduction\\nthe light curve and polarizati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>b' the @xmath0 and @xmath1 meson production in...</td>\n",
       "      <td>b'the alice experiment @xcite scientific progr...</td>\n",
       "      <td>b'introduction\\nresults\\nconclusions\\nreferences'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>b' recently there have been experimental resul...</td>\n",
       "      <td>b'the wave nature of light which explains the ...</td>\n",
       "      <td>b'introduction\\nbabinet principle: poisson spo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>b' we show that it is decidable whether a tran...</td>\n",
       "      <td>b'in the area of model - checking , the search...</td>\n",
       "      <td>b' introduction\\n preliminaries\\na technical l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               abstract  \\\n",
       "0     b' we use the integral - field spectrograph sa...   \n",
       "1     b\" we report on an effort to study the connect...   \n",
       "2     b' we consider the scenario of a magnetic fiel...   \n",
       "3     b' we present a model - independent analysis o...   \n",
       "4     b' we evaluate the feasibility of the implemen...   \n",
       "...                                                 ...   \n",
       "9995  b' clump clusters and chain galaxies in the hu...   \n",
       "9996  b' recent advances in early detection and deta...   \n",
       "9997  b' the @xmath0 and @xmath1 meson production in...   \n",
       "9998  b' recently there have been experimental resul...   \n",
       "9999  b' we show that it is decidable whether a tran...   \n",
       "\n",
       "                                                article  \\\n",
       "0     b'according to the current galaxy formation pa...   \n",
       "1     b\"the interaction between high power plasma je...   \n",
       "2     b'the interstellar medium ( ism ) exhibits str...   \n",
       "3     b'accreting stellar - mass black holes in bina...   \n",
       "4     b'one of the most prominent applications in qu...   \n",
       "...                                                 ...   \n",
       "9995  b\"galaxies observed with the advanced camera f...   \n",
       "9996  b\"within the fireball model for gamma - ray bu...   \n",
       "9997  b'the alice experiment @xcite scientific progr...   \n",
       "9998  b'the wave nature of light which explains the ...   \n",
       "9999  b'in the area of model - checking , the search...   \n",
       "\n",
       "                                          section_names  \n",
       "0     b'introduction\\nsummary and conclusion\\nacknow...  \n",
       "1     b'introduction\\ncomputational methods\\nmodel d...  \n",
       "2     b'introduction\\nfront structure\\nphase transit...  \n",
       "3     b'introduction\\nobservations and timing analys...  \n",
       "4     b'introduction\\nquantum repeater protocol opti...  \n",
       "...                                                 ...  \n",
       "9995  b'introduction\\ndata on udf bulges and clumps\\...  \n",
       "9996  b'introduction\\nthe light curve and polarizati...  \n",
       "9997  b'introduction\\nresults\\nconclusions\\nreferences'  \n",
       "9998  b'introduction\\nbabinet principle: poisson spo...  \n",
       "9999  b' introduction\\n preliminaries\\na technical l...  \n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../data/sample_10k_data_for_visualization.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5203b3a",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9cf2a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36ed4f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "punc = string.punctuation\n",
    "\n",
    "df[\"article\"] = df[\"article\"].apply(eval)\n",
    "df[\"abstract\"] = df[\"abstract\"].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f46c20e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    newString = text.lower()\n",
    "    newString = BeautifulSoup(newString, \"lxml\").text\n",
    "    tokens = [w for w in newString.split() if not w in stop_words]\n",
    "    tokens = [char for char in tokens if not char in punc]\n",
    "    \n",
    "    return (\" \".join(tokens)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed78e302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dark matter haloes e.g. happy dog cat,'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cleaner(\"dark matter haloes e.g. and but happy dog and cat, <h1>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32a1e24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3702/1153800922.py:3: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  newString = BeautifulSoup(newString, \"lxml\").text\n"
     ]
    }
   ],
   "source": [
    "df[\"abstract_clean\"] = df[\"abstract\"].apply(text_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0802a281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3702/1153800922.py:3: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  newString = BeautifulSoup(newString, \"lxml\").text\n"
     ]
    }
   ],
   "source": [
    "df[\"article_clean\"] = df[\"article\"].apply(text_cleaner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71e05bf",
   "metadata": {},
   "source": [
    "## Define Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "851c6a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")\n",
    "\n",
    "max_article_length = 8192\n",
    "max_abstract_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22b1a6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    examples = examples[1]\n",
    "    model_inputs = tokenizer(examples[\"article_clean\"], max_length=max_article_length, \n",
    "                             padding= \"max_length\", truncation=True, return_attention_mask= False, return_token_type_ids= False,\n",
    "                             return_tensors= \"tf\")\n",
    "    \n",
    "    labels = tokenizer(text_target=examples[\"abstract_clean\"], max_length=max_abstract_length, \n",
    "                       padding= \"max_length\", truncation=True, return_tensors=\"tf\")\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173285fd",
   "metadata": {},
   "source": [
    "### Test tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85846a31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba70a21b560e491a868715d9ab308baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_articles = [preprocess_function(row) for row in tqdm(df.iterrows())]\n",
    "\n",
    "# Multiprocess this\n",
    "\n",
    "\n",
    "# pool = multiprocessing.Pool()\n",
    "# result = list(tqdm(pool.map(preprocess_function, df.loc[:200].iterrows()),total_length = df.shape[0]))\n",
    "# pool.close()\n",
    "# pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2482359b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,  8007, 10845, ...,     1,     1,     1], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_articles[1][\"input_ids\"].numpy().reshape(8192,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b422c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_articles[0][\"labels\"].shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a290239c",
   "metadata": {},
   "source": [
    "### Make batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f49f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7edb626",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_tensors = np.array([data['input_ids'].numpy().reshape(8192,) for data in tokenized_articles])\n",
    "abstract_tensors = np.array([data['labels'].numpy().reshape(512,) for data in tokenized_articles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99eac8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((articles_tensors, abstract_tensors))\n",
    "dataset = dataset.shuffle(SHUFFLE_BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c26dfab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ShuffleDataset element_spec=(TensorSpec(shape=(8192,), dtype=tf.int32, name=None), TensorSpec(shape=(512,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "364ed053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0  8007 10845 ...     1     1     1]\n",
      "[    0  7415  1351   892  7070 11043 27361  3188 22703 29051  7964  3611\n",
      "   786 12968 10417 35235  9883  2584  7964  2942    92 37920  3552 35235\n",
      "  4240  2386   203  4271   356   936   678   941 11043  1950   130 43654\n",
      "  7964  3489 16529 29472  3863  4900  2342  3760 26435 33842  4900 15168\n",
      " 30970  6292 19157 25771  4761 17156   747  7677  2621   670  4900 17829\n",
      "  4817  9159  1455  2297 15800  3092 49015 10632  1386   203  2632 45518\n",
      "  4817  3748 12801  4620   583   253  4900  3315   444   540 16887  6184\n",
      " 33100 19308 11401 17512  5447  7964  2200 46132   196   157 20231 11416\n",
      " 39216  2368 33073  3315 35668  2632  6184 17796   611 12179  2839 10662\n",
      " 34265  1879  5580 35235  9883   817  1202   304  3611 35235  8576 40182\n",
      "   400   731 10662     2     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n"
     ]
    }
   ],
   "source": [
    "for i,j in dataset.take(1):\n",
    "    print(i.numpy())\n",
    "    print(j.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b653689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = dataset.take(1000) \n",
    "train_dataset = dataset.skip(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c19dde2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f24d26bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LEDTokenizerFast(name_or_path='allenai/led-base-16384', vocab_size=50265, model_max_length=16384, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b1d6fd",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f78b3000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session() \n",
    "\n",
    "latent_dim= 512\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e849048",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7832067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(max_article_length,))\n",
    "input_embedding = tf.keras.layers.Embedding(vocab_size, latent_dim, trainable=True)(inputs) \n",
    "\n",
    "encoder_lstm1 = tf.keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True) \n",
    "encoder_output1 , state_h1, state_c1 = encoder_lstm1(input_embedding) \n",
    "\n",
    "encoder_lstm2 = tf.keras.layers.LSTM(latent_dim,return_sequences=True,return_state=True) \n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \n",
    "\n",
    "#LSTM 3 \n",
    "encoder_lstm3= tf.keras.layers.LSTM(latent_dim, return_state=True, return_sequences=True) \n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07946a29",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea6164a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = tf.keras.layers.Input(shape=(None,)) \n",
    "dec_emb_layer = tf.keras.layers.Embedding(vocab_size, latent_dim,trainable=True) \n",
    "dec_emb = dec_emb_layer(decoder_inputs) \n",
    "\n",
    "#LSTM using encoder_states as initial state\n",
    "decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True) \n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb, initial_state=[state_h, state_c]) \n",
    "\n",
    "decoder_dense = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(vocab_size, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c6f4e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 8192)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 8192, 512)    25735680    ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 8192, 512),  2099200     ['embedding[0][0]']              \n",
      "                                 (None, 512),                                                     \n",
      "                                 (None, 512)]                                                     \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, 8192, 512),  2099200     ['lstm[0][0]']                   \n",
      "                                 (None, 512),                                                     \n",
      "                                 (None, 512)]                                                     \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 512)    25735680    ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  [(None, 8192, 512),  2099200     ['lstm_1[0][0]']                 \n",
      "                                 (None, 512),                                                     \n",
      "                                 (None, 512)]                                                     \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  [(None, None, 512),  2099200     ['embedding_1[0][0]',            \n",
      "                                 (None, 512),                     'lstm_2[0][1]',                 \n",
      "                                 (None, 512)]                     'lstm_2[0][2]']                 \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, None, 50265)  25785945   ['lstm_3[0][0]']                 \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 85,654,105\n",
      "Trainable params: 85,654,105\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Model([inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df3a2ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7debbac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = tf.keras.utils.split_dataset(\n",
    "    train_dataset, left_size=0.9, right_size=0.1, shuffle=True, seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1799dce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 8192)\n",
      "(32, 512)\n"
     ]
    }
   ],
   "source": [
    "for i, j in train_ds.take(1):\n",
    "    print(i.numpy().shape)\n",
    "    print(j.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d876b204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/rootroot/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/rootroot/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/rootroot/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/rootroot/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/rootroot/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/rootroot/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 8192) dtype=int32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filebr2x8z2r.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/rootroot/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/rootroot/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/rootroot/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/rootroot/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/rootroot/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/rootroot/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 8192) dtype=int32>]\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=50, callbacks=[es], validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "baf285c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.PrefetchDataset"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc88f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
